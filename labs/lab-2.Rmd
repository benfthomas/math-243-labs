---
title: 'Lab 2: Linear Regression'
author: "Ben Thomas"
subtitle: An Island Never Cries
header-includes:
- \usepackage{dcolumn}
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: spacelab
  
---

* * *

### Earthquake detection

#### Exercise 1

```{r}
# Create the first scatterplot 
library(ggplot2)
DataPlot1 <- ggplot(quakes, aes(x=mag, y=stations)) + geom_point(alpha=.2) +
  labs(title="Magnitude vs number of stations reporting", 
         y="Number of stations reporting", x="Richter magnitude") +
  geom_jitter() +
  theme_light() + theme(plot.title = element_text(hjust = 0.5)) 
DataPlot1
```

After plotting the relationship between the magnitude of an earthquake in our sample and how many stations reported detecting it, it seems clear that there is a strong, positive relationship between the two, as we would expect. 

\newpage
#### Exercise 2

If there were absolutely no relationship between a quake's richter magnitude and the number of stations which reported detecting the quake, then I'd expect a linear regression with the number of stations reporting as the output and the quake's magnitude as the input to have an intercept equal to the average number of stations reporting, and a slope of 0. 

#### Exercise 3

Let's test this! Below I've created and summarized a linear regression model (using stargazer for aesthetics), and added it to the plot from before. 

```{r}
# Creating the linear model
m1 <- lm(stations ~ mag, data = quakes)
```

```{r, eval=FALSE}
# Display the regression results with stargazer
stargazer(m1, title="Regression Results", align=TRUE, 
          dep.var.labels =c("Number of stations reporting"), 
          covariate.labels=c("Richter magnitude"),
          omit.stat=c("LL","ser","f"))
```

<!-- Here begins the stargazer LaTeX output. I have not evalutated the output of the R chunk above, because it knits as the direct LaTeX code, rather than as the table. -->
\begin{table}[!htbp] \centering 
  \caption{Regression Results} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & \multicolumn{1}{c}{Number of stations reporting} \\ 
\hline \\[-1.8ex] 
 Richter magnitude & 46.282^{***} \\ 
  & (0.903) \\ 
  & \\ 
 Constant & -180.424^{***} \\ 
  & (4.190) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{1,000} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.725} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.724} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}
<!-- Here ends the stargazer output -->
\newpage
```{r}
# Displaying the original plot with the regression line
DataPlot2 <- ggplot(quakes, aes(x=mag, y=stations)) + 
  geom_point(alpha=.5) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], col = 2) +
  labs(title="Magnitude vs number of stations reporting", 
         y="Number of stations reporting", x="Richter magnitude") +
  geom_jitter() +
  theme_light() + 
  theme(plot.title = element_text(hjust = 0.5)) 
DataPlot2
```

As we can see, the slope of the coefficient of the richter magnitude is distinctly not 0, and the intercept is not the mean of the stations reporting. This indicates that there is a relationship between the magnitude of an earthquake and the number of stations that report it. Specifically, we'd expect that an increase in magnitude of 1 would lead to around 46 more stations reporting the quake. 

#### Exercise 4

While I have faith in R's lm() function, it never hurts to verify the regression output by hand. I've done this below, and as you can see, the slope generated by hand is identical to the one produced by the lm() function earlier. 

```{r}
x <- quakes$mag
y <- quakes$stations
slope <- (sd(y)/sd(x)) * cor(x, y)
slope
```

#### Exercise 5

We can also verify R's calculation of the confidence interval by hand. I've done so below. 

```{r}
SE_slope <- summary(m1)$coef[2,2]
n <- nrow(quakes)
t_stat <- qt(.025, df = n - 2)

LB <- slope + t_stat * SE_slope
UB <- slope - t_stat * SE_slope
c(LB, UB)
```

This calculated confidence interval (44.51 to 48.05) is identical to the one generated by R's confint() function, displayed below.

```{r}
confint(m1)
```

#### Exercise 6

With our regression, we can predict how many stations might detect an earthquake of a given magnitude. Given an earthquake with magnitude 7.0, we can predict that the number of stations which would detect it would be equal to the intercept of our regression equation, plus 7.0 multiplied by the slope of our regression line. This would end up equaling around 144 stations.

#### Exercise 7

- In Exercise 1, our primary goal was to graphically understand and describe the relationship between magnitude and number of stations reporting. This makes our goal there *data description*. 
- In Exercise 2, we attempted to understand the hypothetical scenario where there was no relationship, and stated what our regression output would look like in this scenario. Our goal here was still *data description*. 
- In Exercise 3, our goal was to understand the relationship between quake magnitude and number of stations reporting using regression. Since we were trying to determine whether or not magnitude was important for how many stations report, our main goal was *inference*. 
- In Exercises 4 and 5, we verified the results that R had given us with lm() and confint(). Our goal here was therefore *data description*. 
- In Exercise 6, we aimed to predict the number of stations that would report a magnitude 7 earthquake. This is *prediction*. 

\newpage
### Simulation

#### Exercise 9

In order to assess how appropriate the model created above is for the dataset, I am going to simulate a new dataset using my fitted model, and compare it to the original data. To start, I'm going to generate 1,000 new X values, normally distributed, with a mean magnitude of 5.0. 

```{r}
xNew <- rnorm(1000, 5, .5)
```

#### Exercise 10

Now, I'll generate the predicted y values for each of the new x values, using the linear model created earlier. 

```{r}
# The function to generate the estimated y values
f_hat <- function(x) {
- 180.424 + 46.28221 * x
}

# Generate the new estimated y values
yHatNew <- f_hat(xNew)
```

#### Exercise 11

We'll now generate the new y values. To do so, we'll take our estimated y values, generated above, and add in some error, from our original data. 

```{r}
# Find the standard error from our original linear model
RSS <- sum((m1$res)^2)
sigma2 <- RSS/998
sigma <- sqrt(sigma2)

# Generate our new y values
yNew <- yHatNew + rnorm(1000, 0, sigma)

# Add all new values into a new dataset
newData <- data.frame(xNew, yNew)
```

#### Exercise 12

Let's plot our new dataset. Note that I've also included the old dataset underneath our new one, for easy comparison. Note that I'm using the ggpubr and cowplot packages for aesthetic purposes. 

```{r, message=F, warning=F}
library(cowplot)
library(ggpubr)
newDataPlot <- ggplot(data = newData, aes(x=xNew, y=yNew)) + 
  geom_point(alpha=.2) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], col = 2) +
  labs(title="New magnitude vs number of stations reporting", 
         y="Number of stations reporting", x="Richter magnitude") +
  geom_jitter() +
  theme_light() + 
  theme(plot.title = element_text(hjust = 0.5)) 

ggarrange(newDataPlot, DataPlot2, ncol = 1, nrow = 2, heights = c(8, 8))
```

Clearly, there are some differences between the actual dataset and the one we generated. For one, ours is centered around magnitude 5 and normally distributed, while the original is clustered more around smaller magnitudes, and distinctly not normally distributed. Additionally, as the magnitude increases, the number of stations reporting in the original dataset tends to be above the trend line, which does not hold for our generated dataset. We could potentially get around this by estimating a polynomial regression, and using this new trend line as our mean. 

\newpage
#### Challenge Problem

As an interesting exercise, let's map these earthquakes using latitude and longitude, with the size of the dot indicating the magnitude of the quake. Doing so might inform us as to where the problem areas are, and where residents might need to take more caution. 

```{r}
MapPlot <- ggplot(data = quakes, aes(x=lat, y=long)) + 
  geom_point(size = (1 + .005*(quakes$mag)^4), alpha=.3) +
  labs(title="Earthquake map", 
         y="Longitude", x="Latitude") +
  theme_light() + 
  theme(plot.title = element_text(hjust = 0.5)) 
MapPlot
```

\newpage

# Problem Set 2

### Chapter 3 exercises

#### Problem 1

In Table 3.4, the p-values correspond to the following null hypotheses, respectively: 

- the TV advertising budget has no effect on sales
- the radio advertising budget has no effect on sales
- the newspaper advertising budget has no effect on sales

Based on the p-values reported in this table, we know that:

- there is less than a 1% chance that we would observe the association we see between TV advertising budgets and sales if there were no actual relationship between the two
- there is less than a 1% chance that we would observe the association seen between radio advertising budgets and sales if there were no actual relationship between the two
- there is an 86% chance that we would see the observed relationship between newspaper advertising budgets and sales if there were no actual relationship between the two

From these probabilities, we can therefore conclude that: 

- there is a significant (at the 99% level) relationship between TV advertising budgets and sales
- there is a significant (at the 99% level) relationship between radio advertising budgets and sales
- there is not a significant relationship between newspaper advertising budgets and sales

#### Problem 4

a. For the training data, even though the true relationship in our data is linear, I'd expect the cubic regression to have a lower residual sum of squares. This is because this regression will catch more of the random error, unlike the linear regression, which is not as flexible. 
b. For the test data, I'd expect the linear regression to have a lower residual sum of squares. While the cubic regression would have a lower RSS for the training data, it would accomplish this by overfitting. When faced with a different data set, which is true to the true relationship (which in this case is linear) but which doesn't have the exact pattern of random error seen in the training data, the cubic regression will perform worse than the linear. 
c. No matter the true relationship in the data, I'd expect the cubic regression to have a lower RSS when looking at the training data. As said before, it is more flexible, and accounts for more variation (which may or may not reflect the actual relationship in the data) than the linear model would. 
d. In this case, I think my answer depends on the true relationship inherent in the data. If the true relationship is linear, then as before I'd expect the linear model to have a lower test RSS. However, if the relationship is cubic (or quartic, etc.) then I'd expect the cubic regression to have a lower test RSS. 

\newpage
#### Problem 5

\begin{eqnarray} 
\hat{y}_i = x_i \beta \\
= x_i \frac{\sum_{i' = 1}^n x_{i'} y_{i'}}{\sum_{j=1}^n x_j ^2} \\
= \frac{\sum_{i'=1}^n x_i x_{i'}y_{i'}}{\sum_{j=1}^n x_j ^2} \\
= \sum_{i'=1}^n a_{i'} y_{i'}
\end{eqnarray}
where $$a_{i'} \equiv \frac{x_i x_{i'}}{\sum_{j=1} ^n x_j ^2}$$

### Additional exercise

The equation below describes the k-nearest neighbors estimator. 
$$\hat{f}(x) = \frac{1}{k} \sum_{x_i \in \mathcal{N}(x)} y_i$$
Since this model has a closed form for bias and variance, we can directly calculate them. Let's use the following data set as training data.

```{r, warning=F, message=F}
library(tidyverse)
x <- c(1:3, 5:12)
y <- c(-7.1, -7.1, .5, -3.6, -2, -1.7,
       -4, -.2, -1.2, -1.2, -3.5)
df_train <- tibble(x, y)
```

We know: $$MSE = E((y - \hat{f})^2) = var(\hat{f}) + [f - E(\hat{f})]^2 + var(\epsilon)$$
where $var(\hat{f})$ is the variance, $[f - E(\hat{f})]^2$ is the bias, and $var(\epsilon)$ is the random error term. 

From this information, we can use some identities to isolate each of these terms.
$$MSE = var(\hat{f}) + [f - E(\hat{f})]^2 + var(\epsilon)\\
= var(\frac{1}{k} \sum_{x_i \in \mathcal{N}(x)} y_i) + [f - E(\frac{1}{k} * \sum_{x_i \in \mathcal{N}(x)} y_i)]^2 + \sigma^2\\$$

After some additional math, this boils down to:
$$\frac{\sigma^2}{K} + [f - \frac{1}{K} * \sum_{x_i \in \mathcal{N}(x)} f(x_i)]^2 + \sigma^2$$

After subbing in for our true f, given to us as $f = -9.3 + 2.6 x - 0.3 x^2 + .01 x^3$, and in for $\sigma^2$, equal to 1, then:
$$\frac{1}{K} + [-9.3 + 2.6 x - 0.3 x^2 + .01 x^3 - \frac{1}{K} * \sum_{x_i \in \mathcal{N}(x)} -9.3 + 2.6 x_i - 0.3 x_i^2 + .01 x_i^3]^2 + 1$$

where $\frac{1}{K}$ is the variance, $[-9.3 + 2.6 x - 0.3 x^2 + .01 x^3 - \frac{1}{K} * \sum_{x_i \in \mathcal{N}(x)} (-9.3 + 2.6 x_i - 0.3 x_i^2 + .01 x_i^3)]^2$ is the bias, and $1$ is the random error. 

Then, assuming that $\sigma^2$ is 1:

- our variance is $\frac{1}{K}$
- our bias is $[-9.3 + 2.6 x - 0.3 x^2 + .01 x^3 - \frac{1}{K} * \sum_{x_i \in \mathcal{N}(x)} (-9.3 + 2.6 x_i - 0.3 x_i^2 + .01 x_i^3)]^2$
- our random error is $1$

Let's say x = 5. Then, our bias is:
$$[-3.93- \frac{1}{K} * \sum_{x_i \in \mathcal{N}(x)} (-9.3 + 2.6 x_i - 0.3 x_i^2 + .01 x_i^3)]^2$$
Graphed, this looks like:
```{r}
k <- 1:10

df <- tibble(k = k)
ggplot(df) + geom_line(aes(x = k, y = 1), col = "tomato") +
  geom_line(aes(x = k, y = 1/k), col = "blue") +
  ylab("variability")
```


